---
title: "HW7"
author: "Yigao Li"
date: "April 1, 2018"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 6.8 - 11

## (a)

```{r}
library(MASS)
library(glmnet)
boston <- Boston
Split <- floor(dim(boston[1])*0.78)
train <- 1:Split
test <- (Split+1):dim(boston)[1]
boston.train <- boston[train,]
boston.test <- boston[test,]
xtrain.matrix <- model.matrix(crim ~ ., data = boston.train)
xtest.matrix <- model.matrix(crim ~ ., data = boston.test)
y <- boston.train$crim
lasso.fit <- glmnet(xtrain.matrix, y, alpha = 1)
plot(lasso.fit, xvar = "lambda", lwd = 3)
grid(col = 3)
```

## (b)

```{r}
lasso.cv <- cv.glmnet(xtrain.matrix, y, alpha = 1)
best.lambda.lasso <- lasso.cv$lambda.min
pred.lasso <- predict(lasso.fit, s = best.lambda.lasso, newx = xtest.matrix)
mean((boston.test$crim - pred.lasso)^2)
coef(lasso.cv, s = best.lambda.lasso)
```

## (c)

No, not all variables are used in LASSO because it forces some coefficients to be set to zero.

# 7.9 - 3

```{r}
x <- seq(-2,2,0.01)
y <- 1 + x - 2 * (x-1)^2 * I(x>=1)
plot(x,y)
```

# 7.9 - 9

## (a)

```{r}
model <- lm(nox ~ poly(dis, 3), data = boston)
summary(model)
plot(boston$dis, boston$nox)
dis <- seq(1,12,0.01)
xdf <- data.frame(dis, dis^2, dis^3)
y <- predict(model, xdf)
lines(dis, y, col = "magenta2")
```

## (b)

```{r}
for (i in 1:10){
  polyfit <- lm(nox ~ poly(dis, i), data = boston)
  print(sum(polyfit$residual^2))
}
```

## (c)

```{r}
set.seed(6)
library(boot)
for (i in 1:10){
  polygfit <- glm(nox ~ poly(dis, i), data = boston)
  loocv <- cv.glm(boston, polygfit)
  print(loocv$delta)
}
```

The minimum prediction error is when polynomial degree is 3. Therefore, the optimal degree is 3.

# 7.9 - 10

## (a)

```{r}
library(ISLR)
library(leaps)
college <- College
min.model <- lm(Outstate ~ 1, data = college)
full.model <- lm(Outstate ~ ., data = college)
step(min.model, scope = list(lower = min.model, upper = full.model), direction = "forward")
```

## (b)

```{r}
library(gam)
GAM <- gam(Outstate ~ s(Expend, df = 1) + Private + s(Room.Board, df = 5) + s(perc.alumni, df = 3)
           + s(PhD, df = 1) + s(Grad.Rate, df = 5) + s(Personal, df = 2) + s(Terminal, df = 5)
           + s(S.F.Ratio, df = 4) + s(Accept, df = 1) + s(F.Undergrad, df = 1) + s(Apps, df = 1)
           + s(Top10perc, df = 5) + s(Enroll, df = 1), data = college)
summary(GAM)
```